{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "03d6fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from types import MethodType\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "33b1d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU setups\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ec23d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e54964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from types import MethodType\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def wrap_vit_blocks_dino(model):\n",
    "    activations = {}\n",
    "    original_blocks = model.blocks  \n",
    "\n",
    "    for i, block in enumerate(original_blocks):\n",
    "        def make_custom_forward(orig_forward, layer_name):\n",
    "            def custom_forward(self, x):\n",
    "                out = orig_forward(x)\n",
    "                activations[layer_name] = out.detach()\n",
    "                return out\n",
    "            return custom_forward\n",
    "\n",
    "        block.forward = MethodType(make_custom_forward(block.forward, f\"layer_{i}\"), block)\n",
    "\n",
    "    return activations\n",
    "\n",
    "def logit_lens_analysis_dino(activations, model, final_cls_token, temperature=1.0, true_class_idx=None):\n",
    "    distances = {}\n",
    "    predictions = {}\n",
    "    true_class_probs = []\n",
    "    first_top_class_probs = []\n",
    "    last_layer_top_class_probs = []\n",
    "    last_layer_second_top_class_probs = []\n",
    "    last_layer_third_top_class_probs = []\n",
    "    random_class_probs = []\n",
    "    kl_divergence = []\n",
    "\n",
    "    first_layer_top_class = None\n",
    "    last_layer_top_class = None\n",
    "\n",
    "    last_layer_name = list(activations.keys())[-1]\n",
    "\n",
    "    last_layer_activ = activations[last_layer_name]\n",
    "    last_cls_token = last_layer_activ[:, 0, :]\n",
    "    last_normed = model.norm(last_cls_token)\n",
    "    last_logits = model.head(last_normed)\n",
    "    last_probs = F.softmax(last_logits / temperature, dim=-1)\n",
    "    topk_probs, topk_indices = torch.topk(last_probs, k=3, dim=-1)  \n",
    "\n",
    "    last_layer_top_class = topk_indices[0][0].item()    # top-1\n",
    "    last_layer_second_top_class = topk_indices[0][1].item()  # top-2\n",
    "    last_layer_third_top_class = topk_indices[0][2].item()   # top-3\n",
    "\n",
    "\n",
    "    for i, (name, x) in enumerate(activations.items()):\n",
    "        cls_token = x[:, 0, :]\n",
    "        normed = model.norm(cls_token)\n",
    "\n",
    "        similarity = F.cosine_similarity(normed, final_cls_token, dim=-1)\n",
    "        distances[name] = similarity.detach().cpu().item()\n",
    "\n",
    "        logits = model.head(normed)\n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        if i == len(activations) - 1:\n",
    "            kl_div = torch.zeros(probs.shape[0], device=probs.device)\n",
    "        else:\n",
    "            last_probs = F.softmax(last_logits / temperature, dim=-1)\n",
    "            kl_div = F.kl_div(probs.log(), last_probs, reduction='none').sum(dim=-1)\n",
    "\n",
    "        top_prob, top_class = torch.max(probs, dim=-1)\n",
    "        predictions[f\"{name}_label\"] = int(top_class[0].cpu().item())\n",
    "        predictions[f\"{name}_prob\"] = float(top_prob[0].cpu().item())\n",
    "\n",
    "        if true_class_idx is not None:\n",
    "            true_class_probs.append(float(probs[0, true_class_idx].item()))\n",
    "        else:\n",
    "            true_class_probs[name] = None\n",
    "\n",
    "        if i == 0:\n",
    "            first_layer_top_class = int(top_class[0].cpu().item())\n",
    "\n",
    "        if first_layer_top_class is not None:\n",
    "           first_top_class_probs.append(float(probs[0, first_layer_top_class].item()))\n",
    "        else:\n",
    "            first_top_class_probs[name] = None\n",
    "    \n",
    "        if last_layer_top_class is not None:\n",
    "            last_layer_top_class_probs.append(float(probs[0, last_layer_top_class].item()))\n",
    "        else:\n",
    "            last_layer_top_class_probs.append(None)\n",
    "        \n",
    "        if last_layer_second_top_class is not None:\n",
    "            last_layer_second_top_class_probs.append(float(probs[0, last_layer_second_top_class].item()))\n",
    "        else:\n",
    "            last_layer_second_top_class_probs.append(None)\n",
    "        \n",
    "        if last_layer_third_top_class is not None:\n",
    "           last_layer_third_top_class_probs.append(float(probs[0, last_layer_third_top_class].item()))\n",
    "        else:\n",
    "            last_layer_third_top_class_probs.append(None)\n",
    "\n",
    "        if kl_div is not None:\n",
    "            kl_divergence.append(float(kl_div.mean().item()))\n",
    "    \n",
    "    return distances, predictions, true_class_probs, first_top_class_probs, last_layer_top_class_probs, last_layer_second_top_class_probs, last_layer_third_top_class_probs, kl_divergence, first_layer_top_class, last_layer_top_class, last_layer_second_top_class, last_layer_third_top_class\n",
    "\n",
    "def load_tiny_imagenet_labels(path=\"tiny-imagenet-200/words.txt\"):\n",
    "    wnid_to_label = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            wnid, label = line.strip().split(\"\\t\")\n",
    "            wnid_to_label[wnid] = label\n",
    "    return wnid_to_label\n",
    "\n",
    "def perform_logit_lens_analysis(model, dataset, device,\n",
    "                                cosine_path = \"logit_lens_results/cosine_similarity.csv\", \n",
    "                                preds_path = \"logit_lens_results/predictions.csv\",\n",
    "                                true_probs_path = \"logit_lens_results/true_class_probs.csv\",\n",
    "                                first_top_probs_path = \"logit_lens_results/first_top_class_probs.csv\",\n",
    "                                last_layer_probs_path = \"logit_lens_results/last_layer_top_class_probs.csv\",\n",
    "                                second_last_layer_probs_path = \"logit_lens_results/second_last_layer_top_class_probs.csv\",\n",
    "                                third_last_layer_probs_path = \"logit_lens_results/third_last_layer_top_class_probs.csv\",\n",
    "                                kl_divergence_path = \"logit_lens_results/kl_divergence.csv\"):\n",
    "    model.eval()\n",
    "    idx_to_wnid = {v: k for k, v in dataset.dataset.class_to_idx.items()}\n",
    "\n",
    "    os.makedirs(\"logit_lens_results\", exist_ok=True)\n",
    "\n",
    "    activations = wrap_vit_blocks_dino(model)  \n",
    "    headers = [f\"layer_{i}\" for i in range(len(model.blocks))]\n",
    "    wnid_to_label = load_tiny_imagenet_labels()\n",
    "\n",
    "    for image_idx, (image, label) in enumerate(dataset):\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        true_wnid = idx_to_wnid[label] \n",
    "        true_label = wnid_to_label.get(true_wnid, \"\") \n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.activations = {}  \n",
    "            features = model.forward_features(image)     \n",
    "            cls_token = features[:, 0, :]  \n",
    "            final_cls_token = model.norm(cls_token)     \n",
    "            final_output = model.head(final_cls_token)   \n",
    "\n",
    "            true_class_idx = label.item() if hasattr(label, 'item') else int(label)\n",
    "            distances, predictions, true_class_probs, first_top_probs, last_layer_probs, second_last_layer_probs, third_last_layer_probs, kl_div, first_class_idx, last_layer_top_class_idx, last_layer_second_top_class_idx, last_layer_third_top_class_idx = logit_lens_analysis_dino(\n",
    "                activations, model, final_cls_token, true_class_idx=true_class_idx)\n",
    "            \n",
    "        # --- KL DIVERGENCE FILE ---\n",
    "        os.makedirs(os.path.dirname(kl_divergence_path), exist_ok=True)\n",
    "        kl_header = ['Image', 'True_WNID', 'True_Label'] + headers\n",
    "        write_header = not os.path.exists(kl_divergence_path) or os.path.getsize(kl_divergence_path) == 0\n",
    "        with open(kl_divergence_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(kl_header)\n",
    "            kl_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label] + kl_div\n",
    "            writer.writerow(kl_row)\n",
    "\n",
    "\n",
    "        # --- COSINE FILE ---\n",
    "        os.makedirs(os.path.dirname(cosine_path), exist_ok=True)\n",
    "        cosine_header = ['Image', 'True_WNID', 'True_Label'] + headers\n",
    "        write_header = not os.path.exists(cosine_path) or os.path.getsize(cosine_path) == 0\n",
    "        with open(cosine_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(cosine_header)\n",
    "            cosine_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label] + \\\n",
    "                        [distances[layer] for layer in headers]\n",
    "            writer.writerow(cosine_row)\n",
    "\n",
    "        # --- PREDICTIONS FILE ---\n",
    "        os.makedirs(os.path.dirname(preds_path), exist_ok=True) \n",
    "        pred_header = ['Image', 'True_WNID', 'True_Label'] + \\\n",
    "            [f\"{layer}_label\" for layer in headers] + \\\n",
    "            [f\"{layer}_prob\" for layer in headers]\n",
    "        write_header = not os.path.exists(preds_path) or os.path.getsize(preds_path) == 0\n",
    "        with open(preds_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(pred_header)\n",
    "            pred_probs = [predictions[f\"{layer}_prob\"] for layer in headers]\n",
    "            pred_indices = [predictions[f\"{layer}_label\"] for layer in headers]\n",
    "            pred_wnids = [idx_to_wnid[int(idx)] for idx in pred_indices]\n",
    "            pred_labels = [wnid_to_label.get(wnid, wnid) for wnid in pred_wnids]\n",
    "\n",
    "            pred_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label] + pred_labels + pred_probs\n",
    "            writer.writerow(pred_row)\n",
    "\n",
    "        # --- TRUE CLASS PROBS FILE ---\n",
    "        os.makedirs(os.path.dirname(true_probs_path), exist_ok=True)\n",
    "        true_header = ['Image', 'True_WNID', 'True_Label'] + headers\n",
    "\n",
    "        write_header = not os.path.exists(true_probs_path) or os.path.getsize(true_probs_path) == 0\n",
    "        with open(true_probs_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(true_header)\n",
    "            true_probs_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label] + true_class_probs\n",
    "\n",
    "            writer.writerow(true_probs_row)\n",
    "\n",
    "        # --- FIRST PREDICTED CLASS PROBS FILE ---\n",
    "        os.makedirs(os.path.dirname(first_top_probs_path), exist_ok=True)\n",
    "        first_top_header = ['Image', 'True_WNID', 'True_Label', 'First_Top1_WNID', 'First_Top1_Label'] + headers\n",
    "\n",
    "        # Get first top class info\n",
    "        first_top_idx = predictions.get(f\"{headers[0]}_label\", None)\n",
    "        first_top_wnid = idx_to_wnid.get(first_top_idx, \"N/A\")\n",
    "        first_top_label = wnid_to_label.get(first_top_wnid, \"N/A\")\n",
    "\n",
    "        write_header = not os.path.exists(first_top_probs_path) or os.path.getsize(first_top_probs_path) == 0\n",
    "        with open(first_top_probs_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(first_top_header)\n",
    "            first_probs_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label, first_top_wnid, first_top_label] + first_top_probs  \n",
    "            writer.writerow(first_probs_row)\n",
    "\n",
    "\n",
    "        # --- LAST LAYER TOP CLASS PROBS FILE ---\n",
    "        os.makedirs(os.path.dirname(last_layer_probs_path), exist_ok=True)\n",
    "        last_layer_header = ['Image', 'True_WNID', 'True_Label', 'Last_Top1_WNID', 'Last_Top1_Label'] + headers\n",
    "\n",
    "        last_layer_top_idx = predictions.get(f\"{headers[-1]}_label\", None)\n",
    "        last_layer_top_wnid = idx_to_wnid.get(last_layer_top_idx, \"N/A\")\n",
    "        last_layer_top_label = wnid_to_label.get(last_layer_top_wnid, \"N/A\")\n",
    "        write_header = not os.path.exists(last_layer_probs_path) or os.path.getsize(last_layer_probs_path) == 0\n",
    "        with open(last_layer_probs_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(last_layer_header)\n",
    "            last_probs_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label, last_layer_top_wnid, last_layer_top_label] + last_layer_probs  \n",
    "            writer.writerow(last_probs_row)\n",
    "\n",
    "        # --- SECOND LAST LAYER TOP CLASS PROBS FILE ---\n",
    "        second_last_layer_probs_header = ['Image', 'True_WNID', 'True_Label', 'Second_Last_Top1_WNID', 'Second_Last_Top1_Label'] + headers\n",
    "        second_last_wnid = idx_to_wnid.get(last_layer_second_top_class_idx, \"N/A\")\n",
    "        second_last_label = wnid_to_label.get(second_last_wnid, \"N/A\")\n",
    "        write_header = not os.path.exists(second_last_layer_probs_path) or os.path.getsize(second_last_layer_probs_path) == 0\n",
    "        with open(second_last_layer_probs_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(second_last_layer_probs_header)\n",
    "            second_last_layer_probs_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label, second_last_wnid, second_last_label] + second_last_layer_probs\n",
    "            writer.writerow(second_last_layer_probs_row)\n",
    "            \n",
    "        # --- THIRD LAST LAYER TOP CLASS PROBS FILE ---\n",
    "        third_last_layer_probs_header = ['Image', 'True_WNID', 'True_Label', 'Third_Last_Top1_WNID', 'Third_Last_Top1_Label'] + headers\n",
    "        third_last_wnid = idx_to_wnid.get(last_layer_third_top_class_idx, \"N/A\")\n",
    "        third_last_label = wnid_to_label.get(third_last_wnid, \"N/A\")\n",
    "        write_header = not os.path.exists(third_last_layer_probs_path) or os.path.getsize(third_last_layer_probs_path) == 0\n",
    "        with open(third_last_layer_probs_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(third_last_layer_probs_header)\n",
    "            third_last_layer_probs_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label, third_last_wnid, third_last_label] + third_last_layer_probs\n",
    "            writer.writerow(third_last_layer_probs_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# config\n",
    "RANDOM_SEED = 42\n",
    "SUBSET_FRACTION = 0.1  # 0.05 for 5%, 0.1 for 10%\n",
    "BATCH_SIZE = 64\n",
    "VAL_FRACTION = 0.1 \n",
    "\n",
    "# transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                         std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "\n",
    "train_dir = os.path.join(\"./tiny-imagenet-200\", \"train\")\n",
    "full_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "\n",
    "test_dir = os.path.join(\"./tiny-imagenet-200\", \"test\")\n",
    "full_test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "class_indices = defaultdict(list)\n",
    "for idx, (_, label) in enumerate(full_dataset.samples):\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "# stratified sampling\n",
    "rng = random.Random(RANDOM_SEED)\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "for label, indices in class_indices.items():\n",
    "    rng.shuffle(indices)  \n",
    "    \n",
    "    n_total = len(indices)\n",
    "    n_train = int(SUBSET_FRACTION * n_total)\n",
    "    n_val = int(VAL_FRACTION * n_total)\n",
    "\n",
    "    n_train = max(n_train, 1)\n",
    "    n_val = max(n_val, 1)\n",
    "\n",
    "    available = indices[:n_train + n_val]\n",
    "    train_indices.extend(available[:n_train])\n",
    "    val_indices.extend(available[n_train:n_train + n_val])\n",
    "\n",
    "\n",
    "train_indices = sorted(train_indices)\n",
    "val_indices = sorted(val_indices)\n",
    "\n",
    "train_subset = Subset(full_dataset, train_indices)\n",
    "val_subset = Subset(full_dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1af825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# import timm\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "# MODEL TRAINING\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# num_classes = 200\n",
    "# batch_size = 32\n",
    "# epochs = 10\n",
    "# lr = 0.001\n",
    "\n",
    "\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "# ])\n",
    "\n",
    "# val_transform = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "# ])\n",
    "\n",
    "\n",
    "# model = timm.create_model('vit_small_patch16_224.dino', pretrained=True)\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# num_features = model.num_features \n",
    "\n",
    "# model.head = nn.Linear(model.num_features, num_classes)\n",
    "# for param in model.head.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.head.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     for images, labels in val_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item() * images.size(0)\n",
    "#         _, predicted = outputs.max(1)\n",
    "#         correct += predicted.eq(labels).sum().item()\n",
    "#         total += labels.size(0)\n",
    "\n",
    "#     train_acc = correct / total\n",
    "#     train_loss = total_loss / total\n",
    "#     print(f\"Epoch {epoch+1}/{epochs} - Loss: {train_loss:.4f} - Acc: {train_acc:.4f}\")\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# model_path = \"vit_dino_finetuned.pth\"\n",
    "# torch.save(model.state_dict(), model_path)\n",
    "# print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d3c1a5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "num_classes = 200\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = timm.create_model('vit_small_patch16_224.dino', pretrained=False) \n",
    "num_features = model.num_features \n",
    "model.head = nn.Linear(model.num_features, num_classes)\n",
    "state_dict = torch.load(\"vit_dino_finetuned.pth\", map_location=\"cpu\") \n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "24d8edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_logit_lens_analysis(model=model, dataset=train_subset, device=device,\n",
    "                            cosine_path=\"logit_lens_results/DINO_lp/cosine_similarity.csv\", \n",
    "                            preds_path=\"logit_lens_results/DINO_lp/predictions.csv\", \n",
    "                            true_probs_path=\"logit_lens_results/DINO_lp/true_class_probs.csv\", \n",
    "                            first_top_probs_path=\"logit_lens_results/DINO_lp/first_top_class_probs.csv\", \n",
    "                            last_layer_probs_path=\"logit_lens_results/DINO_lp/last_layer_top_class_probs.csv\", \n",
    "                            second_last_layer_probs_path=\"logit_lens_results/DINO_lp/second_last_layer_top_class_probs.csv\",\n",
    "                            third_last_layer_probs_path=\"logit_lens_results/DINO_lp/third_last_layer_top_class_probs.csv\",\n",
    "                            kl_divergence_path=\"logit_lens_results/DINO_lp/kl_divergence.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
