{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03d6fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from types import MethodType\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4f451d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU setups\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a851f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e54964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_vit_blocks(model):\n",
    "    activations = {}\n",
    "    original_blocks = model.visual.transformer.resblocks\n",
    "\n",
    "    for i, block in enumerate(original_blocks):\n",
    "        def make_custom_forward(orig_block, layer_name):\n",
    "            def custom_forward(self, x):\n",
    "                out = orig_block(x)\n",
    "                activations[layer_name] = out.clone()\n",
    "                return out\n",
    "            return custom_forward\n",
    "\n",
    "        block.forward = MethodType(make_custom_forward(block.forward, f\"layer_{i}\"), block)\n",
    "\n",
    "    return activations\n",
    "\n",
    "\n",
    "def logit_lens_analysis(activations, projection_head, ln_post, final_output, text_features, dictionary, temperature, top_k=1, true_class_idx=None):\n",
    "    distances = {}\n",
    "    predictions = {}\n",
    "    true_class_probs = []\n",
    "    first_top_class_probs = []\n",
    "    last_layer_top_class_probs = []\n",
    "    last_layer_second_top_class_probs = []\n",
    "    last_layer_third_top_class_probs = []\n",
    "    random_class_probs = []\n",
    "    kl_divergence = []\n",
    "\n",
    "    first_layer_top_class = None\n",
    "    last_layer_top_class = None\n",
    "\n",
    "    first_class_idx = None\n",
    "    last_class_idx = None\n",
    "\n",
    "    last_layer_name = list(activations.keys())[-1]\n",
    "    # get probs at the last layer\n",
    "    last_layer_activ = activations[last_layer_name]\n",
    "    last_cls_token = last_layer_activ[:,0,:]\n",
    "    last_cls_token = ln_post(last_cls_token)\n",
    "    last_logits = last_cls_token @ projection_head\n",
    "    last_logits = F.normalize(last_logits, dim=-1)\n",
    "    last_logits = temperature * last_logits @ text_features.T\n",
    "    last_probs = F.softmax(last_logits, dim=-1)\n",
    "    topk_probs, topk_indices = torch.topk(last_probs, k=3, dim=-1)  # zakładam batch_size = 1\n",
    "\n",
    "    last_layer_top_class = topk_indices[0][0].item()    # top-1\n",
    "    last_layer_second_top_class = topk_indices[0][1].item()  # top-2\n",
    "    last_layer_third_top_class = topk_indices[0][2].item()   # top-3\n",
    "\n",
    "    \n",
    "    for name, x in activations.items():\n",
    "        x = x.permute(1, 0, 2)  # -> (batch, seq_len, dim)\n",
    "        cls_token = x[:, 0, :]  # take CLS token\n",
    "\n",
    "        cls_token = ln_post(cls_token)\n",
    "\n",
    "        # [1] Projekcja CLS tokena\n",
    "        projected = cls_token @ projection_head\n",
    "        projected = F.normalize(projected, dim=-1)\n",
    "\n",
    "        # [2] Oblicz logity jak w CLIP\n",
    "        logits = temperature * projected @ text_features.T \n",
    "\n",
    "        # [3] Softmax i top-k\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        top_k_probs, top_k_indices = torch.topk(probs, k=top_k)\n",
    "\n",
    "        # Calculate KL divergence if last layer return zeros\n",
    "        if name == last_layer_name:\n",
    "            kl_div = torch.zeros(logits.shape[-1], device=logits.device)\n",
    "        else:\n",
    "            kl_div = F.kl_div(probs.log(), last_probs, reduction='none')\n",
    "\n",
    "        # [4] Zapisz predykcje\n",
    "        top_k_predictions = []\n",
    "        for idx, prob in zip(top_k_indices[0], top_k_probs[0]):  # [0] bo batch = 1\n",
    "            predicted_idx = idx.item()\n",
    "            predicted_label = dictionary[predicted_idx]\n",
    "            top_k_predictions.append((predicted_label, prob.item()))\n",
    "\n",
    "        predictions[name] = top_k_predictions\n",
    "\n",
    "        # random_class_idx = random.randint(0, logits.shape[-1] - 1)\n",
    "        # if random_class_idx is not None:\n",
    "        #     random_class_probs.append(float(probs[0, random_class_idx].item()))\n",
    "\n",
    "        if true_class_idx is not None:\n",
    "            true_class_probs.append(float(probs[0, true_class_idx].item()))\n",
    "        else: \n",
    "            true_class_probs[name] = None\n",
    "\n",
    "        if name == list(activations.keys())[0]:\n",
    "            first_class_idx = top_k_indices[0][0].item()\n",
    "            first_layer_top_class = top_k_predictions[0][0] if top_k_predictions else None\n",
    "\n",
    "        if first_layer_top_class is not None:\n",
    "            first_top_class_probs.append(float(probs[0, first_class_idx].item()))\n",
    "        else:\n",
    "            first_top_class_probs[name] = None\n",
    "\n",
    "        if last_layer_top_class is not None:\n",
    "            last_layer_top_class_probs.append(float(probs[0, last_layer_top_class].item()))\n",
    "        else:\n",
    "            last_layer_top_class_probs.append(None)\n",
    "\n",
    "        if last_layer_second_top_class is not None:\n",
    "            last_layer_second_top_class_probs.append(float(probs[0, last_layer_second_top_class].item()))\n",
    "        else:\n",
    "            last_layer_second_top_class_probs.append(None)\n",
    "        \n",
    "        if last_layer_third_top_class is not None:\n",
    "           last_layer_third_top_class_probs.append(float(probs[0, last_layer_third_top_class].item()))\n",
    "        else:\n",
    "            last_layer_third_top_class_probs.append(None)\n",
    "\n",
    "        if kl_div is not None:\n",
    "            kl_divergence.append(float(kl_div.mean().item()))\n",
    "        \n",
    "        # [5] Dodatkowo: cosine similarity do final_output — zostaje\n",
    "        similarity = F.cosine_similarity(projected, final_output, dim=-1)\n",
    "        distances[name] = similarity.detach().cpu().numpy().tolist()[0]\n",
    "\n",
    "    return distances, predictions, true_class_probs, first_top_class_probs, last_layer_top_class_probs, last_layer_second_top_class_probs, last_layer_third_top_class_probs, kl_divergence #, first_class_idx, last_class_idx, second_last_class_idx, third_last_class_idx\n",
    "\n",
    "def load_tiny_imagenet_labels(path=\"tiny-imagenet-200/words.txt\"):\n",
    "    wnid_to_label = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            wnid, label = line.strip().split(\"\\t\")\n",
    "            wnid_to_label[wnid] = label\n",
    "    return wnid_to_label\n",
    "\n",
    "def perform_logit_lens_analysis(model, dataset, device, cosine_path = \"logit_lens_results/cosine_similarity.csv\", \n",
    "                                preds_path = \"logit_lens_results/predictions.csv\",\n",
    "                                true_probs_path = \"logit_lens_results/true_class_probs.csv\",\n",
    "                                first_top_probs_path = \"logit_lens_results/first_top_class_probs.csv\",\n",
    "                                last_layer_probs_path = \"logit_lens_results/last_layer_top_class_probs.csv\",\n",
    "                                second_last_layer_probs_path = \"logit_lens_results/second_last_layer_top_class_probs.csv\",\n",
    "                                third_last_layer_probs_path = \"logit_lens_results/third_last_layer_top_class_probs.csv\",\n",
    "                                kl_divergence_path = \"logit_lens_results/kl_divergence.csv\"):\n",
    "    idx_to_class = {v: k for k, v in dataset.dataset.class_to_idx.items()}\n",
    "    model.eval()\n",
    "\n",
    "    os.makedirs(\"logit_lens_results\", exist_ok=True)\n",
    "\n",
    "    prev_activations = activations.copy() if 'activations' in globals() else {}\n",
    "    activations = wrap_vit_blocks(model)\n",
    "\n",
    "    for key in prev_activations:\n",
    "        assert key not in activations, f\"Key {key} from previous activations is still present! Possible accumulation.\"\n",
    "\n",
    "    headers = sorted([f\"layer_{i}\" for i in range(len(model.visual.transformer.resblocks))], key=lambda x: int(x.split('_')[1]))\n",
    "\n",
    "    wnid_to_label = load_tiny_imagenet_labels()\n",
    "    all_classes = [f\"a photo of {wnid_to_label[idx_to_class[i]]}\" for i in range(len(idx_to_class))]\n",
    "    \n",
    "    text_tokens_all = clip.tokenize(all_classes).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_text_features = model.encode_text(text_tokens_all)\n",
    "        all_text_features = all_text_features / all_text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "    for image_idx, (image, label) in enumerate(dataset):\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "\n",
    "        final_output = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        true_wnid = idx_to_class[label] \n",
    "        true_label = wnid_to_label.get(true_wnid, \"\") \n",
    "        true_class_idx = label.item() if hasattr(label, 'item') else int(label)\n",
    "\n",
    "        distances, predictions, true_class_probs, first_top_probs, last_layer_probs, second_last_layer_probs, third_last_layer_probs, kl_div = logit_lens_analysis(\n",
    "            activations,\n",
    "            model.visual.proj,\n",
    "            model.visual.ln_post,\n",
    "            final_output,\n",
    "            all_text_features,\n",
    "            idx_to_class, \n",
    "            model.logit_scale.exp(),\n",
    "            true_class_idx=true_class_idx\n",
    "        )\n",
    "        \n",
    "        os.makedirs(os.path.dirname(cosine_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(preds_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(true_probs_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(first_top_probs_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(last_layer_probs_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(second_last_layer_probs_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(third_last_layer_probs_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(kl_divergence_path), exist_ok=True) \n",
    "        \n",
    "        # --- KL DIVERGENCE FILE ---\n",
    "        kl_divergence_header = ['Image', 'True_WNID', 'True_Label'] + headers\n",
    "        write_header = not os.path.exists(kl_divergence_path) or os.path.getsize(kl_divergence_path) == 0\n",
    "        with open(kl_divergence_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(kl_divergence_header)\n",
    "            kl_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label] + kl_div\n",
    "            writer.writerow(kl_row)\n",
    "\n",
    "        # --- COSINE FILE ---\n",
    "        cosine_header = ['Image', 'True_WNID', 'True_Label'] + headers\n",
    "        write_header = not os.path.exists(cosine_path) or os.path.getsize(cosine_path) == 0\n",
    "        with open(cosine_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(cosine_header)\n",
    "            cosine_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label] + \\\n",
    "                        [distances[layer] for layer in headers]\n",
    "            writer.writerow(cosine_row)\n",
    "\n",
    "        # --- PREDICTIONS FILE ---\n",
    "        pred_header = ['Image', 'True_WNID', 'True_Label'] + [f\"{layer}_label\" for layer in headers] + [f\"{layer}_prob\" for layer in headers]\n",
    "        write_header = not os.path.exists(preds_path) or os.path.getsize(preds_path) == 0\n",
    "        with open(preds_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(pred_header)\n",
    "            pred_wnids = [predictions[layer][0][0] for layer in headers]\n",
    "            pred_labels = [wnid_to_label.get(wnid, wnid) for wnid in pred_wnids]\n",
    "            pred_probs = [predictions[layer][0][1] for layer in headers]\n",
    "            pred_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label] + pred_labels + pred_probs\n",
    "            writer.writerow(pred_row)\n",
    "\n",
    "        # --- TRUE CLASS PROBS FILE ---\n",
    "        true_probs_header = ['Image', 'True_WNID', 'True_Label'] + headers\n",
    "        write_header = not os.path.exists(true_probs_path) or os.path.getsize(true_probs_path) == 0\n",
    "        with open(true_probs_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(true_probs_header)\n",
    "            true_probs_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label] + true_class_probs\n",
    "            writer.writerow(true_probs_row)\n",
    "\n",
    "        # --- FIRST TOP CLASS PROBS FILE ---\n",
    "        first_top_probs_header = ['Image', 'True_WNID', 'True_Label', 'First_Top1_WNID', 'First_Top1_Label'] + headers\n",
    "\n",
    "         # Get first top class info\n",
    "        first_top_idx = predictions.get(f\"{headers[0]}\", None)\n",
    "        first_top_idx = first_top_idx[0][0] if first_top_idx else \"N/A\"\n",
    "        first_top_label = wnid_to_label.get(first_top_idx, \"N/A\")\n",
    "\n",
    "        write_header = not os.path.exists(first_top_probs_path) or os.path.getsize(first_top_probs_path) == 0\n",
    "        with open(first_top_probs_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(first_top_probs_header)\n",
    "            first_top_probs_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label, first_top_idx, first_top_label] + first_top_probs\n",
    "            writer.writerow(first_top_probs_row)\n",
    "\n",
    "        # --- LAST LAYER TOP CLASS PROBS FILE ---\n",
    "        last_layer_probs_header = ['Image', 'True_WNID', 'True_Label', 'Last_Top1_WNID', 'Last_Top1_Label'] + headers\n",
    "        last_layer_top_idx = predictions.get(f\"{headers[-1]}\", None)\n",
    "        last_layer_top_idx = last_layer_top_idx[0][0] if last_layer_top_idx else \"N/A\"\n",
    "        last_label = wnid_to_label.get(last_layer_top_idx, \"N/A\")\n",
    "\n",
    "        write_header = not os.path.exists(last_layer_probs_path) or os.path.getsize(last_layer_probs_path) == 0\n",
    "        with open(last_layer_probs_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(last_layer_probs_header)\n",
    "            last_layer_probs_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label, last_layer_top_idx, last_label] + last_layer_probs\n",
    "            writer.writerow(last_layer_probs_row)\n",
    "\n",
    "        # --- SECOND LAST LAYER TOP CLASS PROBS FILE ---\n",
    "        second_last_layer_probs_header = ['Image', 'True_WNID', 'True_Label', 'Second_Last_Top1_WNID', 'Second_Last_Top1_Label'] + headers\n",
    "        second_last_layer_top_idx = predictions.get(f\"{headers[-2]}\", None)\n",
    "        second_last_layer_top_idx = second_last_layer_top_idx[0][0] if second_last_layer_top_idx else \"N/A\"\n",
    "        second_last_label = wnid_to_label.get(second_last_layer_top_idx, \"N/A\")\n",
    "        write_header = not os.path.exists(second_last_layer_probs_path) or os.path.getsize(second_last_layer_probs_path) == 0\n",
    "        with open(second_last_layer_probs_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(second_last_layer_probs_header)\n",
    "            second_last_layer_probs_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label, second_last_layer_top_idx, second_last_label] + second_last_layer_probs\n",
    "            writer.writerow(second_last_layer_probs_row)\n",
    "            \n",
    "        # --- THIRD LAST LAYER TOP CLASS PROBS FILE ---\n",
    "        third_last_layer_probs_header = ['Image', 'True_WNID', 'True_Label', 'Third_Last_Top1_WNID', 'Third_Last_Top1_Label'] + headers\n",
    "        third_last_layer_top_idx = predictions.get(f\"{headers[-3]}\", None)\n",
    "        third_last_layer_top_idx = third_last_layer_top_idx[0][0] if third_last_layer_top_idx else \"N/A\"\n",
    "        third_last_label = wnid_to_label.get(third_last_layer_top_idx, \"N/A\")\n",
    "        write_header = not os.path.exists(third_last_layer_probs_path) or os.path.getsize(third_last_layer_probs_path) == 0\n",
    "        with open(third_last_layer_probs_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(third_last_layer_probs_header)\n",
    "            third_last_layer_probs_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label, third_last_layer_top_idx, third_last_label] + third_last_layer_probs\n",
    "            writer.writerow(third_last_layer_probs_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "763b603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Config\n",
    "RANDOM_SEED = 42\n",
    "SUBSET_FRACTION = 0.1  # 0.05 for 5%, 0.1 for 10%\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                         std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "\n",
    "# Load full dataset\n",
    "train_dir = os.path.join(\"./tiny-imagenet-200\", \"train\")\n",
    "full_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "\n",
    "# Group indices by class\n",
    "class_indices = defaultdict(list)\n",
    "for idx, (_, label) in enumerate(full_dataset.samples):\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "# Stratified sampling\n",
    "rng = random.Random(RANDOM_SEED)\n",
    "subset_indices = []\n",
    "for label, indices in class_indices.items():\n",
    "    rng.shuffle(indices)  # shuffle within each class\n",
    "    n_select = int(SUBSET_FRACTION * len(indices))\n",
    "    n_select = max(n_select, 1)  # ensure at least 1 per class\n",
    "    subset_indices.extend(indices[:n_select])\n",
    "\n",
    "# Sort for consistent image loading order\n",
    "subset_indices = sorted(subset_indices)\n",
    "\n",
    "# Create subset dataset\n",
    "subset_dataset = Subset(full_dataset, subset_indices)\n",
    "train_loader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d9a7122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load CLIP ViT model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24d8edc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mperform_logit_lens_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubset_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mcosine_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_lens_results/CLIP/cosine_similarity.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mpreds_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_lens_results/CLIP/predictions.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mtrue_probs_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_lens_results/CLIP/true_class_probs.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mfirst_top_probs_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_lens_results/CLIP/first_top_class_probs.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mlast_layer_probs_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_lens_results/CLIP/last_layer_top_class_probs.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                            \u001b[49m\u001b[43msecond_last_layer_probs_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_lens_results/CLIP/second_last_layer_top_class_probs.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mthird_last_layer_probs_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_lens_results/CLIP/third_last_layer_top_class_probs.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mkl_divergence_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_lens_results/CLIP/kl_divergence.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 164\u001b[39m, in \u001b[36mperform_logit_lens_analysis\u001b[39m\u001b[34m(model, dataset, device, cosine_path, preds_path, true_probs_path, first_top_probs_path, last_layer_probs_path, second_last_layer_probs_path, third_last_layer_probs_path, kl_divergence_path)\u001b[39m\n\u001b[32m    161\u001b[39m image = image.unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     image_features = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m final_output = F.normalize(image_features, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    168\u001b[39m true_wnid = idx_to_class[label] \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/clip/model.py:341\u001b[39m, in \u001b[36mCLIP.encode_image\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/clip/model.py:232\u001b[39m, in \u001b[36mVisionTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    229\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_pre(x)\n\u001b[32m    231\u001b[39m x = x.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m x = x.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n\u001b[32m    235\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_post(x[:, \u001b[32m0\u001b[39m, :])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/clip/model.py:203\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mwrap_vit_blocks.<locals>.make_custom_forward.<locals>.custom_forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcustom_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     out = \u001b[43morig_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     activations[layer_name] = out.clone()\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mwrap_vit_blocks.<locals>.make_custom_forward.<locals>.custom_forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcustom_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     out = \u001b[43morig_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     activations[layer_name] = out.clone()\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "    \u001b[31m[... skipping similar frames: wrap_vit_blocks.<locals>.make_custom_forward.<locals>.custom_forward at line 8 (6 times)]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mwrap_vit_blocks.<locals>.make_custom_forward.<locals>.custom_forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcustom_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     out = \u001b[43morig_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     activations[layer_name] = out.clone()\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mwrap_vit_blocks.<locals>.make_custom_forward.<locals>.custom_forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcustom_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m      8\u001b[39m     out = orig_block(x)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     activations[layer_name] = \u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "perform_logit_lens_analysis(model=model, dataset=subset_dataset, device=device,\n",
    "                            cosine_path=\"logit_lens_results/CLIP/cosine_similarity.csv\", \n",
    "                            preds_path=\"logit_lens_results/CLIP/predictions.csv\", \n",
    "                            true_probs_path=\"logit_lens_results/CLIP/true_class_probs.csv\", \n",
    "                            first_top_probs_path=\"logit_lens_results/CLIP/first_top_class_probs.csv\", \n",
    "                            last_layer_probs_path=\"logit_lens_results/CLIP/last_layer_top_class_probs.csv\", \n",
    "                            second_last_layer_probs_path=\"logit_lens_results/CLIP/second_last_layer_top_class_probs.csv\",\n",
    "                            third_last_layer_probs_path=\"logit_lens_results/CLIP/third_last_layer_top_class_probs.csv\",\n",
    "                            kl_divergence_path=\"logit_lens_results/CLIP/kl_divergence.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
