{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d6fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from types import MethodType\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f451d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU setups\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a851f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e54964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_vit_blocks(model):\n",
    "    activations = {}\n",
    "    original_blocks = model.visual.transformer.resblocks\n",
    "\n",
    "    for i, block in enumerate(original_blocks):\n",
    "        def make_custom_forward(orig_block, layer_name):\n",
    "            def custom_forward(self, x):\n",
    "                out = orig_block(x)\n",
    "                activations[layer_name] = out.clone()\n",
    "                return out\n",
    "            return custom_forward\n",
    "\n",
    "        block.forward = MethodType(make_custom_forward(block.forward, f\"layer_{i}\"), block)\n",
    "\n",
    "    return activations\n",
    "\n",
    "\n",
    "def logit_lens_analysis(activations, projection_head, ln_post, final_output, text_features, dictionary, temperature, top_k=1, true_class_idx=None):\n",
    "    distances = {}\n",
    "    predictions = {}\n",
    "    true_class_probs = []\n",
    "    first_top_class_probs = []\n",
    "    last_layer_top_class_probs = []\n",
    "    random_class_probs = []\n",
    "    kl_divergence = []\n",
    "\n",
    "    first_layer_top_class = None\n",
    "    last_layer_top_class = None\n",
    "\n",
    "    first_class_idx = None\n",
    "    last_class_idx = None\n",
    "\n",
    "    last_layer_name = list(activations.keys())[-1]\n",
    "    # get probs at the last layer\n",
    "    last_layer_activ = activations[last_layer_name]\n",
    "    last_cls_token = last_layer_activ[:,0,:]\n",
    "    last_cls_token = ln_post(last_cls_token)\n",
    "    last_logits = last_cls_token @ projection_head\n",
    "    last_logits = F.normalize(last_logits, dim=-1)\n",
    "    last_logits = temperature * last_logits @ text_features.T\n",
    "    last_probs = F.softmax(last_logits, dim=-1)\n",
    "    last_class_idx = last_probs.argmax(dim=-1)[0].item()\n",
    "    last_layer_top_class = last_probs.argmax(dim=-1)[0].item()\n",
    "    \n",
    "    for name, x in activations.items():\n",
    "        x = x.permute(1, 0, 2)  # -> (batch, seq_len, dim)\n",
    "        cls_token = x[:, 0, :]  # take CLS token\n",
    "\n",
    "        cls_token = ln_post(cls_token)\n",
    "\n",
    "        # [1] Projekcja CLS tokena\n",
    "        projected = cls_token @ projection_head\n",
    "        projected = F.normalize(projected, dim=-1)\n",
    "\n",
    "        # [2] Oblicz logity jak w CLIP\n",
    "        logits = temperature * projected @ text_features.T \n",
    "\n",
    "        # [3] Softmax i top-k\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        top_k_probs, top_k_indices = torch.topk(probs, k=top_k)\n",
    "\n",
    "        # Calculate KL divergence if last layer return zeros\n",
    "        if name == last_layer_name:\n",
    "            kl_div = torch.zeros(logits.shape[-1], device=logits.device)\n",
    "        else:\n",
    "            kl_div = F.kl_div(probs.log(), last_probs, reduction='none')\n",
    "\n",
    "        # [4] Zapisz predykcje\n",
    "        top_k_predictions = []\n",
    "        for idx, prob in zip(top_k_indices[0], top_k_probs[0]):  # [0] bo batch = 1\n",
    "            predicted_idx = idx.item()\n",
    "            predicted_label = dictionary[predicted_idx]\n",
    "            top_k_predictions.append((predicted_label, prob.item()))\n",
    "\n",
    "        predictions[name] = top_k_predictions\n",
    "\n",
    "        random_class_idx = random.randint(0, logits.shape[-1] - 1)\n",
    "        if random_class_idx is not None:\n",
    "            random_class_probs.append(float(probs[0, random_class_idx].item()))\n",
    "\n",
    "        if true_class_idx is not None:\n",
    "            true_class_probs.append(float(probs[0, true_class_idx].item()))\n",
    "        else: \n",
    "            true_class_probs[name] = None\n",
    "\n",
    "        if name == list(activations.keys())[0]:\n",
    "            first_class_idx = top_k_indices[0][0].item()\n",
    "            first_layer_top_class = top_k_predictions[0][0] if top_k_predictions else None\n",
    "\n",
    "        if first_layer_top_class is not None:\n",
    "            first_top_class_probs.append(float(top_k_probs[0][0].item()))\n",
    "        else:\n",
    "            first_top_class_probs[name] = None\n",
    "\n",
    "        if last_layer_top_class is not None:\n",
    "            last_layer_top_class_probs.append(float(top_k_probs[0][0].item()))\n",
    "        else:\n",
    "            last_layer_top_class_probs.append(None)\n",
    "\n",
    "        if kl_div is not None:\n",
    "            kl_divergence.append(float(kl_div.mean().item()))\n",
    "        \n",
    "        # [5] Dodatkowo: cosine similarity do final_output â€” zostaje\n",
    "        similarity = F.cosine_similarity(projected, final_output, dim=-1)\n",
    "        distances[name] = similarity.detach().cpu().numpy().tolist()[0]\n",
    "\n",
    "    return distances, predictions, true_class_probs, first_top_class_probs, last_layer_top_class_probs, random_class_probs, random_class_idx, kl_divergence, first_class_idx, last_class_idx\n",
    "\n",
    "def load_tiny_imagenet_labels(path=\"tiny-imagenet-200/words.txt\"):\n",
    "    wnid_to_label = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            wnid, label = line.strip().split(\"\\t\")\n",
    "            wnid_to_label[wnid] = label\n",
    "    return wnid_to_label\n",
    "\n",
    "def perform_logit_lens_analysis(model, dataset, device, cosine_path = \"logit_lens_results/cosine_similarity.csv\", preds_path = \"logit_lens_results/predictions.csv\",\n",
    "                                true_probs_path = \"logit_lens_results/true_class_probs.csv\",\n",
    "                                first_top_probs_path = \"logit_lens_results/first_top_class_probs.csv\",\n",
    "                                random_probs_path = \"logit_lens_results/random_class_probs.csv\",\n",
    "                                last_layer_probs_path = \"logit_lens_results/last_layer_top_class_probs.csv\",\n",
    "                                kl_divergence_path = \"logit_lens_results/kl_divergence.csv\"):\n",
    "    idx_to_class = {v: k for k, v in dataset.dataset.class_to_idx.items()}\n",
    "    model.eval()\n",
    "\n",
    "    os.makedirs(\"logit_lens_results\", exist_ok=True)\n",
    "\n",
    "    prev_activations = activations.copy() if 'activations' in globals() else {}\n",
    "    activations = wrap_vit_blocks(model)\n",
    "\n",
    "    for key in prev_activations:\n",
    "        assert key not in activations, f\"Key {key} from previous activations is still present! Possible accumulation.\"\n",
    "\n",
    "    headers = sorted([f\"layer_{i}\" for i in range(len(model.visual.transformer.resblocks))], key=lambda x: int(x.split('_')[1]))\n",
    "\n",
    "    wnid_to_label = load_tiny_imagenet_labels()\n",
    "    all_classes = [f\"a photo of {wnid_to_label[idx_to_class[i]]}\" for i in range(len(idx_to_class))]\n",
    "    \n",
    "    text_tokens_all = clip.tokenize(all_classes).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_text_features = model.encode_text(text_tokens_all)\n",
    "        all_text_features = all_text_features / all_text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "    for image_idx, (image, label) in enumerate(dataset):\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "\n",
    "        final_output = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        true_wnid = idx_to_class[label] \n",
    "        true_label = wnid_to_label.get(true_wnid, \"\") \n",
    "        true_class_idx = label.item() if hasattr(label, 'item') else int(label)\n",
    "\n",
    "        distances, predictions, true_class_probs, first_top_probs, last_layer_probs, random_class_probs, random_idx, kl_div, first_idx, last_idx = logit_lens_analysis(\n",
    "            activations,\n",
    "            model.visual.proj,\n",
    "            model.visual.ln_post,\n",
    "            final_output,\n",
    "            all_text_features,\n",
    "            idx_to_class, \n",
    "            model.logit_scale.exp(),\n",
    "            true_class_idx=true_class_idx\n",
    "        )\n",
    "\n",
    "        os.makedirs(os.path.dirname(cosine_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(preds_path), exist_ok=True) \n",
    "\n",
    "        # Update headers to include ground truth info\n",
    "        cosine_header = ['Image', 'True_WNID', 'True_Label'] + headers\n",
    "        pred_header = ['Image', 'True_WNID', 'True_Label'] + \\\n",
    "                    [f\"{layer}_label\" for layer in headers] + \\\n",
    "                    [f\"{layer}_prob\" for layer in headers]\n",
    "        \n",
    "        # --- KL DIVERGENCE FILE ---\n",
    "        kl_divergence_header = ['Image', 'True_WNID', 'True_Label'] + headers\n",
    "        write_header = not os.path.exists(kl_divergence_path) or os.path.getsize(kl_divergence_path) == 0\n",
    "        with open(kl_divergence_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(kl_divergence_header)\n",
    "            kl_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label] + kl_div\n",
    "            writer.writerow(kl_row)\n",
    "\n",
    "\n",
    "        # --- COSINE FILE ---\n",
    "        write_header = not os.path.exists(cosine_path) or os.path.getsize(cosine_path) == 0\n",
    "        with open(cosine_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(cosine_header)\n",
    "            cosine_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label] + \\\n",
    "                        [distances[layer] for layer in headers]\n",
    "            writer.writerow(cosine_row)\n",
    "\n",
    "        # --- PREDICTIONS FILE ---\n",
    "        write_header = not os.path.exists(preds_path) or os.path.getsize(preds_path) == 0\n",
    "        with open(preds_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(pred_header)\n",
    "            pred_wnids = [predictions[layer][0][0] for layer in headers]\n",
    "            pred_labels = [wnid_to_label.get(wnid, wnid) for wnid in pred_wnids]\n",
    "            pred_probs = [predictions[layer][0][1] for layer in headers]\n",
    "            pred_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label] + pred_labels + pred_probs\n",
    "            writer.writerow(pred_row)\n",
    "\n",
    "        # --- TRUE CLASS PROBS FILE ---\n",
    "        true_probs_header = ['Image', 'True_WNID', 'True_Label'] + headers\n",
    "        write_header = not os.path.exists(true_probs_path) or os.path.getsize(true_probs_path) == 0\n",
    "        with open(true_probs_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(true_probs_header)\n",
    "            true_probs_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label] + true_class_probs\n",
    "            writer.writerow(true_probs_row)\n",
    "\n",
    "        # --- FIRST TOP CLASS PROBS FILE ---\n",
    "        first_top_probs_header = ['Image', 'True_WNID', 'True_Label', 'First_Top1_WNID', 'First_Top1_Label'] + headers\n",
    "\n",
    "        first_top_wnid = idx_to_class.get(first_idx, \"N/A\")\n",
    "        first_top_label = wnid_to_label.get(first_top_wnid, \"N/A\")\n",
    "\n",
    "        write_header = not os.path.exists(first_top_probs_path) or os.path.getsize(first_top_probs_path) == 0\n",
    "        with open(first_top_probs_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(first_top_probs_header)\n",
    "            first_top_probs_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label, first_top_wnid, first_top_label] + first_top_probs\n",
    "            writer.writerow(first_top_probs_row)\n",
    "\n",
    "        # --- LAST LAYER TOP CLASS PROBS FILE ---\n",
    "        last_layer_probs_header = ['Image', 'True_WNID', 'True_Label', 'Last_Top1_WNID', 'Last_Top1_Label'] + headers\n",
    "        last_wnid = idx_to_class.get(last_idx, \"N/A\")\n",
    "        last_label = wnid_to_label.get(last_wnid, \"N/A\")\n",
    "\n",
    "        write_header = not os.path.exists(last_layer_probs_path) or os.path.getsize(last_layer_probs_path) == 0\n",
    "        with open(last_layer_probs_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(last_layer_probs_header)\n",
    "            last_layer_probs_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label, last_wnid, last_label] + last_layer_probs\n",
    "            writer.writerow(last_layer_probs_row)\n",
    "\n",
    "        # --- RANDOM CLASS PROBS FILE ---\n",
    "        random_probs_header = ['Image', 'True_WNID', 'True_Label', 'Random_WNID', 'Random_Label'] + headers\n",
    "\n",
    "        random_wnid = idx_to_class.get(random_idx, \"N/A\")\n",
    "        random_label = wnid_to_label.get(random_wnid, \"N/A\")\n",
    "\n",
    "        write_header = not os.path.exists(random_probs_path) or os.path.getsize(random_probs_path) == 0\n",
    "        with open(random_probs_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(random_probs_header)\n",
    "            random_probs_row = [f\"Image_{image_idx + 1}\", true_wnid, true_label, random_wnid, random_label] + random_class_probs\n",
    "            writer.writerow(random_probs_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "763b603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Config\n",
    "RANDOM_SEED = 42\n",
    "SUBSET_FRACTION = 0.1  # 0.05 for 5%, 0.1 for 10%\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                         std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "\n",
    "# Load full dataset\n",
    "train_dir = os.path.join(\"./tiny-imagenet-200\", \"train\")\n",
    "full_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "\n",
    "# Group indices by class\n",
    "class_indices = defaultdict(list)\n",
    "for idx, (_, label) in enumerate(full_dataset.samples):\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "# Stratified sampling\n",
    "rng = random.Random(RANDOM_SEED)\n",
    "subset_indices = []\n",
    "for label, indices in class_indices.items():\n",
    "    rng.shuffle(indices)  # shuffle within each class\n",
    "    n_select = int(SUBSET_FRACTION * len(indices))\n",
    "    n_select = max(n_select, 1)  # ensure at least 1 per class\n",
    "    subset_indices.extend(indices[:n_select])\n",
    "\n",
    "# Sort for consistent image loading order\n",
    "subset_indices = sorted(subset_indices)\n",
    "\n",
    "# Create subset dataset\n",
    "subset_dataset = Subset(full_dataset, subset_indices)\n",
    "train_loader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d9a7122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load CLIP ViT model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_logit_lens_analysis(model=model, dataset=subset_dataset, device=device, cosine_path=\"logit_lens_results/CLIP/cosine_similarity.csv\", preds_path=\"logit_lens_results/CLIP/predictions.csv\", true_probs_path=\"logit_lens_results/CLIP/true_class_probs.csv\", last_layer_probs_path=\"logit_lens_results/CLIP/last_layer_top_class_probs.csv\", first_top_probs_path=\"logit_lens_results/CLIP/first_top_class_probs.csv\", random_probs_path=\"logit_lens_results/CLIP/random_class_probs.csv\", kl_divergence_path=\"logit_lens_results/CLIP/kl_divergence.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
