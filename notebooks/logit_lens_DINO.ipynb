{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03d6fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from types import MethodType\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e54964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from types import MethodType\n",
    "import os\n",
    "import csv\n",
    "\n",
    "activations = {}\n",
    "\n",
    "def wrap_vit_blocks_dino(model):\n",
    "    \"\"\"\n",
    "    Złap wyjścia z kolejnych bloków ViT w DINO (timm model).\n",
    "    \"\"\"\n",
    "    activations.clear()\n",
    "    original_blocks = model.blocks  # DINO (timm ViT) ma bloków w model.blocks\n",
    "\n",
    "    for i, block in enumerate(original_blocks):\n",
    "        def make_custom_forward(orig_forward, layer_name):\n",
    "            def custom_forward(self, x):\n",
    "                out = orig_forward(x)\n",
    "                activations[layer_name] = out.detach()\n",
    "                return out\n",
    "            return custom_forward\n",
    "\n",
    "        block.forward = MethodType(make_custom_forward(block.forward, f\"layer_{i}\"), block)\n",
    "\n",
    "    return activations\n",
    "\n",
    "\n",
    "def logit_lens_analysis_dino(activations, projection_head, final_output, temperature=1.0, top_k=5):\n",
    "    \"\"\"\n",
    "    Analiza logit lens dla DINO: porównanie projekcji CLS tokena z finalnym embeddingiem.\n",
    "    Nie ma tekstowego słownika (bo DINO nie ma tekstu),\n",
    "    więc zwracamy tylko cosine similarity i top-k największe wartości z final output.\n",
    "    \"\"\"\n",
    "    distances = {}\n",
    "    predictions = {}\n",
    "\n",
    "    for name, x in activations.items():\n",
    "        # x: (batch, seq_len, dim)\n",
    "        cls_token = x[:, 0, :]  # CLS token\n",
    "        projected = projection_head(cls_token) \n",
    "        projected = F.normalize(projected, dim=-1)\n",
    "\n",
    "        cls_projected = projected[:, 0, :] if projected.ndim == 3 else projected  \n",
    "        cls_final = final_output[:, 0, :] if final_output.ndim == 3 else final_output\n",
    "\n",
    "        similarity = F.cosine_similarity(cls_projected, cls_final, dim=-1)\n",
    "\n",
    "        distances[name] = similarity.detach().cpu().item()\n",
    "\n",
    "        predictions[name] = [(0, similarity.detach().cpu().item())]\n",
    "\n",
    "    return distances, predictions\n",
    "\n",
    "\n",
    "def perform_logit_lens_analysis(model, dataset, device, cosine_path=\"logit_lens_results/cosine_similarity.csv\", preds_path=\"logit_lens_results/predictions.csv\"):\n",
    "    model.eval()\n",
    "    os.makedirs(\"logit_lens_results\", exist_ok=True)\n",
    "\n",
    "    wrap_vit_blocks_dino(model)\n",
    "\n",
    "    headers = [f\"layer_{i}\" for i in range(len(model.blocks))]\n",
    "\n",
    "    for image_idx, (image, label) in enumerate(dataset):\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            final_output = model.forward_features(image) \n",
    "            final_output = F.normalize(final_output, dim=-1)\n",
    "\n",
    "            distances, predictions = logit_lens_analysis_dino(\n",
    "                activations,\n",
    "                model.head if hasattr(model, 'head') else model.proj,  \n",
    "                final_output\n",
    "            )\n",
    "\n",
    "        os.makedirs(os.path.dirname(cosine_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(preds_path), exist_ok=True) \n",
    "\n",
    "        cosine_header = ['Image'] + headers\n",
    "        pred_header = ['Image'] + [f\"{layer}_label\" for layer in headers] + [f\"{layer}_prob\" for layer in headers]\n",
    "\n",
    "        write_header = not os.path.exists(cosine_path) or os.path.getsize(cosine_path) == 0\n",
    "        with open(cosine_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(cosine_header)\n",
    "            cosine_row = [f\"Image_{image_idx + 1}\"] + [distances[layer] for layer in headers]\n",
    "            writer.writerow(cosine_row)\n",
    "\n",
    "        write_header = not os.path.exists(preds_path) or os.path.getsize(preds_path) == 0\n",
    "        with open(preds_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow(pred_header)\n",
    "            pred_labels = [predictions[layer][0][0] for layer in headers] \n",
    "            pred_probs = [predictions[layer][0][1] for layer in headers] \n",
    "            pred_row = [f\"Image_{image_idx + 1}\"] + pred_labels + pred_probs\n",
    "            writer.writerow(pred_row)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_results(distances, predictions):\n",
    "    '''\n",
    "    Plots cosine similarity and prediction probabilities for each layer\n",
    "    '''\n",
    "    layer_names = sorted(\n",
    "        [k for k in distances.keys() if k.startswith(\"layer_\")],\n",
    "        key=lambda x: int(x.split('_')[1])\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        similarity_values = [float(distances[layer]) for layer in layer_names]\n",
    "        prob_values = [float(predictions.get(f\"{layer}_prob\", np.nan)) for layer in layer_names]\n",
    "        predicted_labels = [predictions.get(f\"{layer}_label\", \"\") for layer in layer_names]\n",
    "    except Exception as e:\n",
    "        print(\"Błąd podczas przygotowywania danych:\", e)\n",
    "        return\n",
    "\n",
    "    if not all(isinstance(v, (int, float)) for v in similarity_values + prob_values):\n",
    "        print(\"Niektóre wartości nie są liczbami.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 6), gridspec_kw={'height_ratios': [1, 1]})\n",
    "\n",
    "    sns.heatmap(np.array(similarity_values).reshape(1, -1), annot=True, cmap=\"viridis\",\n",
    "                xticklabels=layer_names, yticklabels=[\"Cosine Similarity\"], cbar=True,\n",
    "                ax=axes[0], cbar_kws={'label': 'Cosine Similarity'})\n",
    "\n",
    "    sns.heatmap(np.array(prob_values).reshape(1, -1), annot=True, cmap=\"magma\",\n",
    "                xticklabels=layer_names, yticklabels=[\"Prediction Prob.\"], cbar=True,\n",
    "                ax=axes[1], cbar_kws={'label': 'Prediction Probability'})\n",
    "\n",
    "    for i, label in enumerate(predicted_labels):\n",
    "        if label: \n",
    "            axes[1].text(i + 0.5, -0.3, label, ha='center', va='center',\n",
    "                         color='black', fontsize=9, rotation=90,\n",
    "                         transform=axes[1].transData)\n",
    "\n",
    "    plt.suptitle(\"Cosine Similarity & Prediction Probability per Layer\", fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "763b603c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Tiny ImageNet...\n",
      "Download and extraction complete.\n"
     ]
    }
   ],
   "source": [
    "url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "extract_path = \"./tiny-imagenet-200\"\n",
    "\n",
    "print(\"Downloading Tiny ImageNet...\")\n",
    "response = requests.get(url)\n",
    "with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "    zip_ref.extractall(\".\")\n",
    "\n",
    "print(\"Download and extraction complete.\")\n",
    "\n",
    "\n",
    "train_dir = os.path.join(extract_path, \"train\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                        std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a7122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = timm.create_model('vit_small_patch16_224.dino', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24d8edc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mperform_logit_lens_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosine_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_lens_results/DINO/cosine_similarity.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_lens_results/DINO/predictions.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mperform_logit_lens_analysis\u001b[39m\u001b[34m(model, dataset, device, cosine_path, preds_path)\u001b[39m\n\u001b[32m     68\u001b[39m image = image.unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# Embedding końcowe (normalizowane)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     final_output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# DINO ma forward_features zwracające CLS token po projekcji\u001b[39;00m\n\u001b[32m     73\u001b[39m     final_output = F.normalize(final_output, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     75\u001b[39m     \u001b[38;5;66;03m# Teraz zrób logit lens dla każdej warstwy\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/timm/models/vision_transformer.py:834\u001b[39m, in \u001b[36mVisionTransformer.forward_features\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    832\u001b[39m     x = checkpoint_seq(\u001b[38;5;28mself\u001b[39m.blocks, x)\n\u001b[32m    833\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m834\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm(x)\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mwrap_vit_blocks_dino.<locals>.make_custom_forward.<locals>.custom_forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcustom_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     out = \u001b[43morig_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     activations[layer_name] = out.detach()\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mwrap_vit_blocks_dino.<locals>.make_custom_forward.<locals>.custom_forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcustom_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     out = \u001b[43morig_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     activations[layer_name] = out.detach()\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "    \u001b[31m[... skipping similar frames: wrap_vit_blocks_dino.<locals>.make_custom_forward.<locals>.custom_forward at line 19 (3 times)]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mwrap_vit_blocks_dino.<locals>.make_custom_forward.<locals>.custom_forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcustom_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     out = \u001b[43morig_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     activations[layer_name] = out.detach()\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/timm/models/vision_transformer.py:169\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.drop_path1(\u001b[38;5;28mself\u001b[39m.ls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m    170\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.drop_path2(\u001b[38;5;28mself\u001b[39m.ls2(\u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.norm2(x))))\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/wb-env/lib/python3.11/site-packages/timm/models/vision_transformer.py:93\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     90\u001b[39m q, k = \u001b[38;5;28mself\u001b[39m.q_norm(q), \u001b[38;5;28mself\u001b[39m.k_norm(k)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fused_attn:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     x = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_drop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     98\u001b[39m     q = q * \u001b[38;5;28mself\u001b[39m.scale\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "perform_logit_lens_analysis(model=model, dataset=train_dataset, device=device, cosine_path=\"logit_lens_results/DINO/cosine_similarity.csv\", preds_path=\"logit_lens_results/DINO/predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "distances = pd.read_csv(\"logit_lens_results/DINO/cosine_similarity.csv\")\n",
    "predictions = pd.read_csv(\"logit_lens_results/DINO/predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d889e41a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
